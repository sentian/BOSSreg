---
title: "Best orthogonalized subset selection (BOSS)"
author: "Sen Tian"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

## Installation
We maintain a Github page for package and keep the most updated version there. 
To install, simply run the following command in the console:
```{r, eval=FALSE}
library(devtools)
install_github(repo="sentian/boss", subdir="r-package")
```
A stable version can be installed from CRAN using
```{r, eval=FALSE}
install.packages(repo="boss", repos = "http://cran.us.r-project.org")
```

## Introduction
BOSS is a least squares based subset selection method. It briefly takes the following steps:

* order the predictors based on their partial correlations with the response, 

* perform best subset regression upon the orthogonal basis of the ordered predictors,

* transform the coefficients back to the original space,

* choose the optimal solution using the selection rule AICc-hdf.

The hdf is a heuristic degrees of freedom for BOSS that can be plugged into a selection rule such as AICc-hdf, 
which can further be used as a selection rule for BOSS. AICc-hdf is defined as
\begin{equation*}
\text{AICc-hdf} = n \log\left(\frac{\text{RSS}}{n}\right) + n \frac{n+\text{hdf}}{n-{hdf}-2},
\end{equation*}
More details can be referred to our paper 

[Tian, Hurvich and Simonoff (2019), On the use of information criterion in least squares based subset selection problems.](https://github.com/sentian/boss/paper/main.pdf)

This vignette is structured as follows. We start by simulating a dataset. We then introduces the components, 
functionalities and basic usage of the package. It is followed by a discussion between BOSS and forward stepwise regression (FS).
Finally, we study real data examples and compare BOSS with some popular regularization methods.

## A small simulated dataset
The model generating mechanism is $y=X\beta+\epsilon$. We consider a sparse model where only a few predictors matter, 
and a high signal-to-noise ratio. The detailed parameters are given as:
```{r}
n = 200 # number of observations
p = 14 # number of predictors
p0 = 6 # number of active predictors (beta_j=0)
rho = 0.9 # correlation between predictors
nrep = 1000 # number of replications of y to be generated
SNR = 7 # signal-to-noise ratio
```

We make the predictors with $\beta_j \ne 0$ pairwise correlated with opposite effects.
Replications of the response $y$ are generated by fixing $X$. We assume the columns of $X$ and $y$
have $0$ mean, so we can exclude the intercept term from model fitting.
```{r}
library(MASS)
# function to generate the data
# columns of X have mean 0 and norm 1, y has mean 0
simu.data <- function(n, p, p0, rho, nrep, SNR){
  # true beta
  beta = rep(0,p)
  beta = c(rep(c(1,-1),p0/2),rep(0,p-p0))
  names(beta) = paste0('X', seq(1,p))
  # covariance matrix
  covmatrix  = matrix(0,nrow=p,ncol=p)
  diag(covmatrix) = 1
  for(i in 1:(p0/2)){
    covmatrix[2*i-1,2*i] = covmatrix[2*i,2*i-1] = rho
  }
  # generate the predictors given the correlation structure
  set.seed(65)
  x = mvrnorm(n,mu=rep(0,p),Sigma=covmatrix)
  x = scale(x,center=TRUE,scale=FALSE)
  colnorm = apply(x,2,function(m){sqrt(sum(m^2))}) 
  x = scale(x,center=FALSE,scale=colnorm) # standardization
  # sigma calculated based on SNR
  sd = sqrt(t(beta/colnorm)%*%covmatrix%*%(beta/colnorm) / SNR)
  mu = x%*%beta 
  # generate replications of y by fixing X
  y = matrix(rep(mu,each=nrep),ncol=nrep,byrow=TRUE) +      
    scale(matrix(rnorm(n*nrep,mean=0,sd=sd),nrow=n,ncol=nrep),center=TRUE,scale=FALSE)
  return(list(x=x, y=y, beta=beta, sigma=sd))
}
dataset = simu.data(n, p, p0, rho, nrep, SNR)
x = dataset$x
y = dataset$y
beta = dataset$beta
mu = x%*%beta
sigma = dataset$sigma
```

The first $p_0=6$ predictors are active with $\beta_j \ne 0$. 
```{r}
print(beta)
```

## Quick start
Fitting the model is simple.
```{r}
library(boss)
# choose a single replication as illustration
rep = 1
# fit the model
boss_model = boss(x, y[,rep], intercept = FALSE)
```

The 'boss' object contains estimated coefficient vectors for entire solution paths of both BOSS and FS.
```{r}
betahat = boss_model$beta_boss
print(dim(betahat))
```

By default, it also calculates the hdf for BOSS and multiple information criteria. 
```{r, fig.width=3, fig.height=3,fig.show='hold'}
# the heuristic degrees of freedom
plot(0:p, boss_model$hdf, main='hdf', ylab='', xlab='subset size')
abline(0,1)
# AICc-hdf
plot(0:p, boss_model$IC_boss$aicc, main='AICc-hdf', ylab='', xlab='subset size')
```

The optimal estimated coefficient vector and fitted mean vector can be obtained as follows.
```{r}
# the default is chosen by AICc
betahat_aicc = coef(boss_model)
muhat_aicc = predict(boss_model, newx=x)
# use Cp rather than AICc
betahat_cp = coef(boss_model, ic='cp')
muhat_cp = predict(boss_model, newx=x)
```

Besides information criterion, cross-validation (CV) can be used as a selection rule.
```{r}
# the default is 10-fold CV with 1 replication
boss_cv_model = cv.boss(x, y[,rep], intercept=FALSE)
# coefficient vector selected by minimizing CV error
betahat_cv = coef(boss_cv_model)
# fitted values
muhat_cv = predict(boss_cv_model, newx=x)
```

Calling 'cv.boss' runs CV for FS as well.
```{r}
# coefficient vector for FS selected by CV
betahat_fs_cv = coef(boss_cv_model, method='fs')
# fitted values
muhat_fs_cv = predict(boss_cv_model, newx=x, method='fs')
```

Let's compare the coefficient vectors selected using different selection rules. The first 
three columns are for BOSS while the last column is for FS.
```{r}
tmp = cbind(betahat_aicc, betahat_cp, betahat_cv, betahat_fs_cv)
dimnames(tmp) = list(dimnames(tmp)[[1]], c('boss_aicc', 'boss_cp', 'boss_cv', 'boss_fs_cv'))
print(tmp)
```

## Compare the solution paths of BOSS and FS

We see that FS gives a denser solution than BOSS. In fact, under the specific design of the true model, 
the true active predictors are pairwise correlated with opposite effects. Predictors say $(X_1,X_2)$ together 
lead to a high $R^2$ but each single one of them contributes little.
Therefore, FS has trouble stepping in all the true active predictors in the early stage. For example,
as indicated below, the inactive predictor $X_9$ joins in the first step. This can potentially lead to overfit 
since the subset containing all $p_0=6$ active predictors may have size larger than $p_0$. 
```{r}
print(boss_model$steps_fs)
```

Let's set aside the selection rule for now, and 



## Real data examples




