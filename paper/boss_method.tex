%!TEX root = ms.tex
\section{Best orthogonalized subset selection (BOSS)}
\label{sec:boss}

For predictors in general position, BS is not computationally feasible for large problems. In this section, we propose a feasible LS-based subset selection method BOSS that is based on orthogonalizing the predictors. The orthogonalization not only makes the computation of BS feasible, but also allows us to take advantage of the superior performance of AICc-hdf as discussed in Section \ref{sec:aicc_performance_bs}. BOSS using AICc-hdf as the selection rule has computational cost of the order $O(npK)$, that is of the same order as a multiple regression on all $p$ predictors for the problem where $n>p$, and on a selected subset of $n$ predictors for $n \le p$. We further demonstrate the competitive performance of BOSS via simulations and real world examples. 

The main steps for BOSS can be summarized as follows: 1) order and orthogonalize the predictors, 2) perform BS on the set of orthogonalized predictors, 3) transform the coefficients back to the original space, and 4) use a selection rule such as AICc-hdf to choose the optimal single subset. 

\subsection{The solution path of BOSS}
\label{sec:boss_solutionpath}

BOSS starts by ordering and orthogonalizing the predictors, taking $K$ steps in total. The ordering is based on partial correlations with the response $y$, and the orthogonalization is based on QR decomposition with Gram-Schmidt. For step $k$, we use $X_{S_{k-1}}$ to denote the set of ordered predictors from the previous step, and use $Q_{S_{k-1}}$ to denote the orthogonal basis of $X_{S_{k-1}}$. From the remaining $p-k+1$ predictors, we choose the one that has the largest correlation with $y$ conditioning on $Q_{S_{k-1}}$, that is the correlation between $y$ and the residual from regressing a candidate predictor on $Q_{S_{k-1}}$. This costs $O(n)$ since we maintain the regression result, e.g. estimated coefficients and residual, in the previous steps. Repeating the above step for all $p-k+1$ predictors costs $O(n(p-k+1))$. We then update the QR decomposition, by adding the chosen predictor as a new column, which costs $O(n(K-k))$ via the modified Gram-Schmidt algorithm as discussed in \citet{hammarling2008updating}. After $K$ steps, we end up with an ordered set of predictors $X_{S_K}$ and its orthogonal basis $Q_{S_K}$, and the total cost for ordering and orthogonalization is $O(npK)$. We denote the regression coefficient vector of $y$ on $Q_{S_K}$ as $z$. BOSS then performs BS on $Q_{S_K}$, which is a ranking of predictors based on their absolute values of corresponding element in $z$, and the cost is $O(K\log(K))$. We use $\tilde{\gamma}(k_Q)$ to denote the BS coefficient vector at size $k_Q$, where $k_Q=1,\cdots,K$ specifies the subset size in the orthogonal space. Finally, BOSS transforms the coefficient vectors $\tilde{\gamma}=[\tilde{\gamma}(0),\cdots,\tilde{\gamma}(K)]$ back to the original space. Therefore, the total cost for the entire solution path of BOSS is on the order of $O(npK)$. The detailed implementation for obtaining the solution path is summarized as steps 1-5 in Algorithm \ref{alg:boss}.

% main algorithm
\begin{algorithm}
	\caption{Best Orthogonalized Subset Selection (BOSS)}\label{alg:boss}
	\begin{enumerate}[label=\arabic*.]
		\item Standardize $y$ and the columns of $X$ to have mean $0$, and denote the means as $\bar{X}$ and $\bar{y}$.
		%\item Denote $S_k$ as the set of predictors that are active at step $k$ and let $S={1,\cdots, p}$.

		\textbf{Order and orthogonalize the predictors:}

		\item From the $p$ predictors, select the one that has the largest marginal correlation with the response $y$,  and denote it as $X_{S_1}$. Standardize $X_{S_1}$ to have unit $l_2$ norm and denote it as $Q_{S_1}$, where $S_1$ is the variable number of this chosen predictor. Calculate $R_{S_1}$ such that $X_{S_1} = Q_{S_1} R_{S_1}$. Let $S=\{1,\cdots, p\}$. Initialize vectors $\text{resid}_j=X_j$ where $j=1,\cdots,p$.
		\item For $k=2,\cdots,K$ ($K=\min\{n,p\}$):
		\begin{enumerate}[label=\alph*.]
			\item For each of the $p-k+1$ predictors $X_j$ in $X_{S \setminus S_{k-1} }$, calculate its partial correlations with the response $y$ conditioning on $Q_{S_{k-1}}$. 
			\iffalse
			\begin{enumerate}[label=a\arabic*.]
				\item Regress $y$ on $q$, augment $Z$ with the coefficient $z$. Set $\text{resid1} = \text{resid1} - zq$.
			\end{enumerate}
			\fi
			%\begin{enumerate}[resume*,label=a\arabic*.]
			\begin{enumerate}[label=a\arabic*.]
				%\item Regress $x_j$ on $q$, call the coefficient $r$. Set $\text{resid2}_j = \text{resid2}_j - rq$.
				%\item Calculate the correlation between resid1 and $\text{resid2}_j$.
				\item Regress $X_j$ on $Q_{S_{k-1} \setminus S_{k-2}}$ ($S_{k-2}=\emptyset$ if $k=2$), and denote the estimated coefficient as $r$. Update $\text{resid}_j = \text{resid}_j - r Q_{S_{k-1} \setminus S_{k-2}}$.
				\item Calculate the correlation between y and $\text{resid}_j$.
			\end{enumerate}
			\item Select the predictor that has the largest partial correlation in magnitude, augment $S_{k-1}$ with this predictor number and call it $S_{k}$.
			\item Update $Q_{S_{k-1}}$ and $R_{S_{k-1}}$ given the newly added column $X_{S_k \setminus S_{k-1}}$, and call them $Q_{S_k}$ and $R_{S_k}$. The update is based on the modified Gram-Schmidt algorithm as discussed in \citet{hammarling2008updating}.
		\end{enumerate}
		
		\textbf{BS on the orthogonalized predictors $Q_{S_{K}}$:}

		\item Calculate $\tilde{\gamma}_j (k_Q)= z_j \mathbbm{1}(|z_j| \ge |z_{(k_Q)}|)$, i.e. the $j$-th component of coefficient vector for subset size $k_Q$, where $z=Q_{S_{K}}^T y$ and $z_{(k_Q)}$ is the $k$-th largest entry in absolute values. Let $\tilde{\gamma} = [\tilde{\gamma} (0) \tilde{\gamma} (1) \cdots \tilde{\gamma} (K)]$.
		
		\textbf{Transform back to the original space:}

		\item Project $\tilde{\gamma}$, a $p \times (K+1)$ matrix, to the original space of $X_{S_{K}}$, i.e. back solving $R \tilde{B} = \tilde{\gamma}$, and re-order the rows of $\tilde{B}$ to their correspondences in $X$, i.e. $\hat{B} = O \tilde{B}$ where $O$ represents the ordering matrix s.t. $X_{S_{K}}=XO$. The intercept vector is $\hat{B}_0 = \bar{y} \mathbbm{1} - \hat{B}^T \bar{X}$. 

		\textbf{Select the subset:}

		\item Select the subset using AICc-hdf (replacing edf with hdf in \eqref{eq:aicc_edf}), where hdf is calculated via Algorithm \ref{alg:hdf}, by inputting $(Q_{S_{K}},\hat\sigma,\hat\mu)$. For $n > p$, we use OLS estimates based on the full model, i.e. $\hat{\mu}=Q_{S_{K}} z$, $\hat{\sigma}^2 = \lVert y-\hat{\mu} \rVert_2^2/(n-p)$. For $n \le p$, we use $\hat{\mu}=\hat{\mu}_l$ and $\hat{\sigma} = \sqrt{\lVert y-\hat{\mu}_l \rVert_2^2 / (n-\text{df}(\hat{\mu}_l)-1)}$ as discussed in \citet{reid2016study}, where $\hat{\mu}_l$ are the lasso fitted values based on 10-fold CV and df$(\hat{\mu}_l)$ is the corresponding number of non-zero coefficients in the lasso estimate. Note that the inclusion of an intercept term implies that hdf is increased by $1$. 
	\end{enumerate}
\end{algorithm}


The ordering of predictors is an important part of implementation of the algorithm. Consider a sparse true model with only two uncorrelated predictors $X=[X_1,X_2]$, $\beta=[0,1]^T$ and a high SNR. The best model in such a scenario is LS regression on $X_2$. Without the ordering step, the orthogonal basis is $Q=[Q_1,Q_2]$ s.t. $X=QR$, i.e. the predictors are orthogonalized in their physical orders. The one-predictor model ($k_Q=1$) of BS can either be $Q_1$ or $Q_2$, which when transformed back to the space of $X$ do not correspond to LS regression upon $X_2$. The former corresponds to LS estimates upon $X_1$, while the latter is a linear combination of LS estimates upon $X$ and LS estimates upon $X_1$; the former leads to a completely wrong model while the latter results in non-zero coefficients on both predictors. In contrast, if $X_2$ is the first variable orthogonalized, the best subset will be based on that variable alone, the correct choice. Therefore, the ordering step is crucial to both sparsity as well as predictive performance. It is worth noting that BOSS is flexible in terms of the choice of ordering rules. For instance, a less aggressive rule based on the lars algorithm \citep{efron2004least} could be adopted instead. We leave discussion of the benefits and drawbacks of different ordering rules to future work. Note that we show that the coefficients of BOSS can be expressed as a linear combination of the LS coefficients on subsets of $X$ in Theorem \ref{thm:correspondence}, the proof of which can be found in the Supplemental Material Section \ref{sec:correspondence}. 


\begin{theorem}[Coefficients of BOSS are a linear combination of LS coefficients on subsets of $X$] 
	Suppose $X$ has full column rank and the columns are already ordered, i.e. $X=X_{S_{K}}$. $X=QR$ where $Q$ is an $n \times K$ matrix with orthonormal columns and $R$ is a $K \times K$ upper-triangular matrix. Let $S_k=\{j_1,j_2,\cdots,j_{k_Q}\}$ denote the support (position of predictors) of the best $k_Q$-predictor model given by BS upon $(Q,y)$, and use $\hat{\gamma}(k_Q)$ ($K$ by $1$) to denote the BS coefficients. The corresponding coefficients in the $X$ space, i.e. $\hat{\beta}(k_Q)$ s.t. $R\hat{\beta}(k_Q)=\hat{\gamma}(k_Q)$, can be expressed as
	\begin{equation*}
	\hat{\beta}(k_Q) = \sum_{j\in S_k} \left(\hat{\alpha}^{(j)} - \hat{\alpha}^{(j-1)}\right),
	\end{equation*}
	where the first $j$ entries in $\hat{\alpha}^{(j)}$ ($K$ by $1$) are LS coefficients of regressing $y$ upon $[X_1,X_2,\cdots,X_j]$ (the first $j$ columns in $X$), and the remaining $K-j$ entries are zero.

	\label{thm:correspondence}
\end{theorem}

\subsection{Connection to FS and the advantage of BOSS}

BOSS is closely related to FS. In fact, instead of performing BS on the set of orthogonalized predictors $Q_{S_{K}}$ in the fourth step of Algorithm \ref{alg:boss}, if we fit LS on the subset of $Q_{S_{K}}$ in a nested fashion, i.e. $\tilde{\gamma}_j (k_Q)= z_j \mathbbm{1}(j \le k_Q)$ and $z=Q_{S_{K}}^T y$, steps 1-5 of the algorithm provide the solution path of FS, and is similar to the orthogonal greedy algorithm discussed in \citet{ing2011stepwise}. Since BS on orthogonal predictors $Q_{S_K}$ is essentially ranking the predictors based on their LS coefficients ($O(K\log(K))$ operations), BOSS involves little additional computational cost compared to FS.

BOSS can, however, provide a better solution path than FS. At a given step, once a predictor is selected, it remains in the subsets of every following step of FS. In many circumstances, the greedy characteristic can lead to overfit, since noise predictors (those with $\beta_j=0$) step in during early steps. However, BS on the set of orthogonalized predictors gives the chance for BOSS to ``look back'' at the predictors that are already stepped in. By revisiting these predictors and allowing them to be dropped, BOSS can provide a solution path that is sparser, with better predictive performance compared to FS. 

We consider two numerical examples Sparse-Ex3 and Sparse-Ex4, where the true models are sparse. The true coefficient vectors for Sparse-Ex3 and Sparse-Ex4 are $\beta=[1_6,0_{p-6}]^T$ and $\beta=[1,-1,5,-5,10,-10,0_{p-6}]^T$, respectively. We consider a high SNR and a high correlation between predictors ($\rho=0.9$). For Sparse-Ex3, the signal predictors (those with non-zero coefficients) are pairwise correlated with noise predictors (correlation coefficient is denoted as $\rho$), while for Sparse-Ex4, the signal predictors are pairwise correlated with opposite effects. We generate the design matrix $X$ once, and draw $1000$ replications of the response $y$ based on \eqref{eq:truemodel_def}. The details of the model setup are given in Supplemental Material Section \ref{sec:simulation_setup_generalx}.

Figure \ref{fig:lossratio_fs_boss_k} shows the average RMSE along the solution paths of BS, FS and BOSS, for the two examples. When the true model is Sparse-Ex3, all three methods provide almost the same solution path. However, for Sparse-Ex4, we see a clear advantage of BOSS over FS in early steps up until about the fifteenth step. Recall that in Sparse-Ex4, there are $p_0=6$ predictors with $\beta_j \ne 0$ that are pairwise correlated with opposite effects, where each pair together leads to a high $R^2$ but each single one of them contributes little. When the correlation between the variables is high, the effect of one almost completely cancels out the effect of the other on $y$. Therefore all of the predictors (both true and noise predictors) have approximately zero marginal correlation with $y$, and they have equal chance of stepping in. Since the subsets along the solution path of FS are nested, if a noise predictor steps in during early steps, it remains in the subsets of every following step, and hence the subset containing both variables in the pair may appear in a late stage. In contrast, BOSS takes ordered predictors provided by FS, and re-orders them by performing BS upon their orthogonal basis, which gives a greater chance for both variables in the pair to appear early in the solution path of BOSS, and potentially results in a better predictive performance than FS. Furthermore, in this example, we note that BOSS provides a better solution path than BS until step $5$ (except the fourth step), and the two methods give similar performances in further steps.

% {fig:lossratio_fs_boss_k}
\begin{figure}[ht!]
	\centering
	\includegraphics[width=\textwidth]{figures/rmse_solpath_lsmethods.eps}
	\caption{RMSE at each subset size, average over $1000$ replications. Note that for BOSS, the subset size $k_Q$ denotes the number of non-zero coefficients in $\tilde{\gamma}(k_Q)$. In both scenarios, we have $n=200$, $p=30$, $\rho=0.9$ and high SNR.}
	\label{fig:lossratio_fs_boss_k}
\end{figure}


\subsection{AICc-hdf as the selection rule for BOSS}

We apply AICc-hdf to choose the single optimal subset from the $K+1$ candidates. The implementation is discussed in step 6 of Algorithm \ref{alg:boss}. The hdf is calculated via Algorithm \ref{alg:hdf} based on the orthogonalized predictors $Q_{S_K}$. As to the estimation of $\mu$ and $\sigma$, if $n>p$, we use the OLS estimates based on the full model, i.e. $\hat{\mu}=Q_{S_{K}} z$, $\hat{\sigma}^2 = \lVert y-\hat{\mu} \rVert_2^2/(n-p)$. If $n \le p$, we use the estimates based on the lasso fit as discussed in \citet{reid2016study}, i.e. $\hat{\mu}=\hat{\mu}_l$ and $\hat{\sigma} = \sqrt{\lVert y-\hat{\mu}_l \rVert_2^2 / (n-\text{df}(\hat{\mu}_l)-1)}$, where $\hat{\mu}_l$ are the lasso fitted values based on 10-fold CV and df$(\hat{\mu}_l)$ is the corresponding number of non-zero coefficients in the lasso estimate. 

A numerical justification of using hdf is given in Figure \ref{fig:boss_aicc_hdf_kl}, where we compare averages of AICc-hdf and $\widehat{\text{Err}}_{\text{KL}}$ over $1000$ replications for BOSS under various true models. The sparse model (Sparse-Ex3) has $p_0=6$ predictors with non-zero coefficients, and all of the predictors in the dense model have non-zero coefficients ($p_0=p$). The correlation between predictors is $\rho=0.5$. We see that by using the sample average to represent the population mean, $E$(AICc-hdf) generally tracks the expected KL, $E(\text{Err}_{\text{KL}})$, reasonably well. Discrepancies can be observed at subset size $k<p_0$, where the set of true predictors is not entirely included in the model. The derivations of the classic AIC and AICc (both with ndf plugged in according to our notation) are based on an assumption that the true predictors are included in the model. In the situation where this assumption is violated, AICc will no longer be unbiased, and a similar conjecture can be made here for AICc in the context of BOSS. Last and most importantly, AICc-hdf yields the same average selected size as $\widehat{\text{Err}}_{\text{KL}}$ across all scenarios.

We find similar evidence in the Supplemental Material Figure \ref{fig:boss_cp_edf_hdf} that $E(\text{C}_p\text{-hdf})$ tracks the expected prediction error $E(\text{Err}_\text{SE})$ well in most cases, and they lead to the same average selected subset size; further supporting the use of hdf for BOSS. As discussed in Section \ref{sec:aicc_performance_bs}, using C$_p$ as the selection rule can perform poorly in practice because of the need to estimate $\sigma$. Evidence of a similar property when using C$_p$ as the selection rule for BOSS can be found in the Online Supplemental Material. For this reason, we prefer AICc in feasible versions of selection. 

% {fig:boss_aicc_hdf_kl}
\begin{figure}[ht!]
	\centering
	\includegraphics[width=0.9\textwidth]{figures/aicc_hdf_kl_boss.eps}
	\caption{Averages of AICc-hdf and $\widehat{\text{Err}}_{\text{KL}}$ for BOSS over $1000$ replications. Here $X$ is general with $n=200$, $p=14$. Both criteria result in the same average of the selected subset size over the $1000$ replications (rounded to the nearest integer) as denoted by the dashed vertical lines. }
	\label{fig:boss_aicc_hdf_kl}
\end{figure}


\subsection{The performance of BOSS in simulations}
\label{sec:boss_performance}
We now study the performance of BOSS via simulations. Besides the above mentioned Sparse-Ex3, Sparse-Ex4 and Dense designs, we consider two additional sparse model designs that have different correlation structures between predictors. For all of the sparse examples, we take $p_0=6$. We further consider three levels of the correlations, $\rho \in [0,0.5,0.9]$, and twelve combinations of $(n,p)$, resulting in a total of $540$ configuration options. For each configuration, $1000$ replications are constructed and we present similar evaluation measures as introduced in Section \ref{sec:aicc_performance_bs}. One measure is the $\%$ worse than the best possible BOSS, where the best possible BOSS means that on a single fit, we choose the subset size $k_Q$ with the minimum RMSE among all $K+1$ candidates, as if an oracle tells us the best model. The details of the simulation setup are discussed in the Supplemental Material Section \ref{sec:simulation_setup_generalx}. The full set of results can be found in the Online Supplemental Material.

We looked at results using AICc-hdf, C$_p$-hdf and 10-fold CV for BOSS, and AICc-hdf performed the best (see Online Supplemental Material), so results for that method are presented here. For FS, we studied several information criteria that have been proposed in the literature, e.g. EBIC \citep{wang2009forward}, HDBIC and HDHQ \citep{ing2011stepwise}. None of these criteria provide strong enough penalties for larger subset sizes. For high dimensional problems ($n<p$), they tend to choose subsets with size close to $n$, which is far from the truth for sparse designs ($p_0=6$). To remedy this overfitting problem, \citet{ing2011stepwise} suggests using a stopping rule, and one example is to only consider subsets with size $k < 5\sqrt{n / \log(p)}$. We find that this ad-hoc stopping rule avoids the problem of not penalizing larger subsets sufficiently, and provides reasonably good performance for sparse designs. However, for dense designs, using the stopping rule gives subsets that substantially underfit. It is clear that using a fixed stopping rule is not appropriate for all problems. For this reason, we prefer 10-fold CV as the selection rule for FS, and we find it overall outperforms these suggested information criteria (see Online Supplemental Material). Finally, we fit BS via the ``leaps'' algorithm for the case where $p \le 30$, and use 10-fold CV as the selection rule. BOSS and FS are fitted using our {\tt{R}} package \pkg{BOSSReg}\footnote{\url{https://github.com/sentian/BOSSreg}. A stable version of the R package is available on \textit{CRAN}.}, and the BS is fitted using the {\tt{R}} package \pkg{leaps} \citep{FortrancodebyAlanMiller2020}.

We also consider some popular regularization methods, including lasso \citep{Tibshirani1996}, SparseNet \citep{Mazumder2011} and gamma lasso \citep{Taddy2017}. We use the {\tt{R}} packages \pkg{glmnet} \citep{Friedman2010}, \pkg{sparsenet} \citep{Mazumder2011}, and \pkg{gamlr} \citep{Taddy2017}, respectively, to fit them, which are all available on \textit{CRAN}. We also consider a simplified version of the relaxed lasso \citep{Meinshausen2007}, which was discussed in \citet{Hastie2017} and can be fitted using the {\tt{R}} package \pkg{bestsubset}\footnote{The package is available at https://github.com/ryantibs/best-subset. We appreciate Prof. Ryan Tibshirani for the suggestion of fitting the simplified relaxed lasso.}. As to the selection rule, we use AICc for lasso, and 10-fold CV for the rest. In addition to these selectors, we have also considered 10-fold CV for lasso. We find (in the Online Supplement) that 10-fold CV performs similarly to AICc for lasso. In fact, the use of AICc for lasso was explored in \citet{Flynn2013}, where the authors proved that AICc is asymptotically efficient while performing similarly to CV. We further notice (results given in the Online Supplement) that SparseNet generally performs better than the relaxed lasso and gamma lasso, and hence only the results for SparseNet will be presented here. 

Note that there is an extensive list of regression estimators existing in the literature and the list is still growing fast. For instance, recent studies by \citet{hazimeh2020fast} and \citet{bertsimas2020sparse} considered regularized best subset problems that combine a lasso or ridge penalty with the cardinality constraint (either in the Lagrangian or constrained form), and the authors provided fast solvers by using modern optimization tools. We expect the general conclusions below regarding the relative performance of BOSS and regularization methods hold for the new methods. Our simulation code \footnote{The code is available at \url{https://github.com/sentian/BOSSreg}.} is structured to be easily extendable, and we invite interested readers to perform further comparisons.


A selected set of simulation results is presented in Table \ref{tab:boss_regu} and \ref{tab:boss_regu_highdim}. Here is a brief summary of the results:


\begin{itemize}
	\item For BOSS, AICc-hdf has a significant advantage over CV in terms of predictive performance, except for low SNR and $n$ is small, in which case the selection rules are comparable. CV is also ten times heavier in terms of computation than AICc-hdf. These results are similar to the comparison of AICc-hdf and CV for BS with an orthogonal $X$ as discussed in Section \ref{sec:aicc_performance_bs}. Overall, the simulations indicate that AICc with hdf used in place of edf is a reasonable selection rule for an LS-based method that can be applied in practice without the requirement that the predictors are orthogonal. In the following discussions, when we refer to BOSS, we mean BOSS-AICc-hdf. 

	\item The performance of BOSS is comparable to the performance of BS when BS is feasible. With a small sample size $n=200$, BOSS performs either similarly to or better than BS for a high SNR, and it performs either similarly to or slightly worse than BS for a low SNR. With a large sample size $n=2000$, BOSS is generally better than BS. Furthermore, BOSS only requires fitting the procedure once while BS uses CV as the selection rule, and a single fit of BOSS only has computational cost $O(npK)$ so that BOSS is feasible for high dimensional problems.

	\item The performance of BOSS is generally better than the performance of FS. In the Dense model, and Sparse-Ex3 with $n=200$ and low SNR, we see that BOSS performs similarly to FS. In all other scenarios, the advantage of BOSS is obvious. For example, in Sparse-Ex4 with $n=200$, high SNR and $\rho=0.9$, FS is almost ten times worse than BOSS in terms of RMSE. Recall that Sparse-Ex4 is an example where FS has trouble stepping in all of the true predictors (with $\beta_j \ne 0$) in early steps. This is evidenced by the fact that FS chooses eight extra predictors on average in this situation, while BOSS only chooses approximately two extra predictors. Furthermore, FS based on CV is ten times computationally heavier than BOSS. 

	\item Compared to the regularization methods, with a small sample size $n=200$, BOSS is the best when SNR is high, lasso is the best when SNR is low and SparseNet is in between. The lasso has the property of ``over-select and shrink,'' in order to retain less bias on the large non-zero estimates. In a high SNR, this property can result in disastrous performance, especially when $p$ is large. For example, in Sparse-Ex3, high SNR, $\rho=0.5$ and $p=180$, the relative efficiency of lasso is only $0.43$ and it significantly overfits. However, this property can be beneficial when SNR is low, as a method like BS has higher chance to miss the true predictors (less sparsistency). With a large sample size $n=550$ and $n=2000$, BOSS is almost always the best even when SNR is low. 

	\item In terms of support recovery in the sparse true models, LS-based methods can recover the true predictors (those with $\beta_j \ne 0$) and rarely include any noise predictors (those with $\beta_j = 0$) when SNR is high or the sample size $n$ is large. However, SparseNet and lasso generally overfit, with the latter being worse in that regard. In the low SNR and small $n$ scenario, lasso and SparseNet have more opportunity to recover the true predictors, but it comes with a price of including more false positives. 

\end{itemize}


Given the spirit of the summary above, it is important to point out the relevant work of \citet{Hastie2017}, where the authors provide a comprehensive set of simulation comparisons on BS, lasso and relaxed lasso. The authors concluded that BS performs the best in high SNR, lasso is the best in low SNR while relaxed lasso is in between. Given the similarity we have noticed between BOSS and BS, it is not surprising that this coincides with our results for BOSS here when sample size is relatively small ($n=200$). However, we find BOSS to be the best for large sample size $n$ even when the SNR is low (note that \citet{Hastie2017} did not examine any sample sizes greater than $n=500$). Moreover, it should be noted that \citet{Hastie2017} focus on the best possible performance of each method by applying a separate validation set drawn from the true model, rather than on feasible selection, as is considered in this study. 


% tab:boss_regu
\input{tables/boss_regu.tex}
% tab:boss_regu_highdim
\input{tables/boss_regu_highdim.tex}


\subsection{The performance of BOSS in real data analysis}
\label{sec:real_data}

We implement BOSS on five real datasets. We consider four datasets from the StatLib library\footnote{http://lib.stat.cmu.edu/datasets/}, which is maintained at Carnegie Mellon University. The ``Housing'' data are often used in comparisons of different regression methods. The aim is to predict the housing values in the suburbs of Boston based on $13$ predictors, including crime rate, property tax rate, pupil-teacher ratio, etc. The ``Hitters'' data contain the 1987 annual salary for MLB players. For each player, it records $19$ different performance metrics happening in 1986, such as number of times at bat, number of hits, etc., and the task is to predict the salary based on these statistics. The ``Auto'' data are driven by prediction of the miles per gallon of vehicles based on features like the horsepower, weight, etc. The ``College'' data contain various statistics for a large number of US colleges from the 1995 issue of ``US News and World Report'', and we use these statistics to predict the number of applications received. We also consider a dataset from the Machine Learning Repository\footnote{https://archive.ics.uci.edu/ml} that is maintained by UC Irvine. The ``ForestFire'' data are provided by \citet{cortez2007data} and the aim is to use meteorological and other data to predict the burned area of forest fires that happened in the northeast region of Portugal. The authors considered several machine learning algorithms, e.g. support vector regression, and concluded that the best prediction in terms of RMSE is the naive mean vector.

%The 'AirFoil' data has different size airfoils at different wind tunnel speeds and angles of attack, based on experiments performed by NASA, and the purpose is to predict sound pressure level. 

In real data analysis, one almost always would consider an intercept term. The way that BOSS handles the intercept term is described in steps 5-6 of Algorithm \ref{alg:boss}. Specifically, we first center both $X$ and $y$, and fit BOSS using AICc-hdf without an intercept to get $\hat{\beta}$. Then we calculate the intercept by $
\hat{\beta}_0=\bar{y} - \bar{X}^T \hat{\beta}$, which can be easily shown to be equivalent to fitting an intercept in every subset considered by BOSS. 

We compare the performance of BOSS with LS-based methods BS and FS, and with regularization methods lasso and SparseNet. All of the methods are fitted with an intercept term. Note that for the Forest Fires dataset, we fit BS via MIO \citep{Bertsimas2016} using the {\tt{R}} package \pkg{bestsubset} \citep{Hastie2017}, where we restrict subset size $k=0,\dots,10$, with $3$ minutes as the time budget to find an optimal solution for each $k$, as suggested by the authors. For all of the other datasets, BS is fitted using the \pkg{leaps} package. To measure the performance of each method, we apply the leave-one-out (LOO) testing procedure, in which we fit the method on all observations except one, test the performance on that particular observation, and repeat the procedure for all $n$ observations. 

Table \ref{tab:realdata} presents the average RMSE, the average number of predictors and average running time for various methods given by LOO testing. We see that BOSS provides the best predictive performance in all datasets except the ``Housing'' and ``College'' data where lasso is the best for those datasets and its RMSE is $0.3\%$ and $0.04\%$ lower than those of BOSS, respectively. Due to a highly optimized implementation of the cyclical coordinate descent, the ``glmnet'' algorithm is extremely fast in providing the lasso solution. BS is still not scalable to large dimensions, even by using the modern tools. With the dimension $p=55$, it takes around $350$ seconds to perform 10-fold CV for subset sizes restricted to be no greater than $10$. However, we observe that BOSS is reasonably computationally efficient and much faster than BS, FS and SparseNet. 

% tab:realdata
\input{tables/realdata.tex}

