%!TEX root = ms.tex
\beginsupplement
\appendix
\pagenumbering{arabic}
\begin{center}
\textbf{\large Supplementary Materials \\
On the Use of Information Criteria for Subset Selection in Least Squares Regression}

Sen Tian, Clifford M. Hurvich, Jeffrey S. Simonoff
\end{center}

\section{Technical details}
\subsection{Proof of theorem \ref{thm:hdf_ydf_representation} and its corollary}
\label{sec:proof_hdf_ydf}
In this section, we assume an orthogonal $X$ and a null true model. This is the only scenario under which both df$_C(k)$ and hdf$(k)$ have analytical expressions. We will prove that the ratio of df$_C(k)$ and hdf$(k)$ goes to $1$ as $k,p\rightarrow \infty$ while $k=\left \lfloor{xp}\right \rfloor $, where $\left \lfloor{\cdot}\right \rfloor$ denotes the greatest integer function and $x\in(0,1)$. We start by laying out a few lemmas to be used in the proof of the main theorem.
\begin{lemma}
	\label{lemma:hdf_nulltrue}
	Assume the design matrix is orthogonal and the true model is null ($\mu=0$). Then
	\begin{equation}
	\text{hdf}(k) = df_L(\lambda_k^\star) = k - 2p\cdot \Phi^{-1} \left(\frac{k}{2p}\right) \cdot \phi\left[\Phi^{-1}\left(\frac{k}{2p}\right) \right].
	\label{eq:hdf_nulltrue}
	\end{equation}
\end{lemma}
\begin{proof}
	We follow the steps described in algorithm \ref{alg:hdf}. We first find $\lambda_k^\star$ from \eqref{eq:thdf_size_expression}, by using the fact that $\mu=0$, and we get $\displaystyle -\frac{\sqrt{2\lambda_k^\star}}{\sigma} = \displaystyle \Phi^{-1}\left(\frac{k}{2p}\right)$, which we then substituted into \eqref{eq:thdf_expression} to get \eqref{eq:hdf_nulltrue}.
\end{proof}

\iffalse
\begin{lemma}
	\label{lemma:hdf_limitends}
	Assume the design matrix is orthogonal and the true model is null ($\mu=0$), with a fixed $p$, and by treating $k$ as continuous, we have
	\begin{equation}
	\lim_{k\to 0} \text{hdf}(k) = 0 \quad \text{and} \quad \text{hdf}(p) = p.
	\end{equation}
\end{lemma}
\begin{proof}
	By \eqref{eq:hdf_nulltrue}, we have
	\begin{equation}
	\text{hdf}(p) = p - 2p\cdot \Phi^{-1} (\frac{1}{2}) \cdot \phi \left[\Phi^{-1}(\frac{1}{2}) \right]=p,
	\end{equation}
	since $\Phi^{-1} (\frac{1}{2})=0$. Meanwhile, 
	\begin{equation}
	\begin{aligned}
	\lim_{k\to 0} \text{hdf}(k) &= \lim_{k\to 0} - 2p\cdot \Phi^{-1} (\frac{k}{2p}) \cdot \phi\left[\Phi^{-1}(\frac{k}{2p}) \right],\\
	&= \lim_{x \to -\infty} -2p \cdot x \cdot \phi(x),\\
	&= \lim_{x \to -\infty} -2p \cdot x \cdot \frac{1}{\sqrt{2\pi}} \exp^{-x^2/2},\\
	&= 0,
	\end{aligned}
	\end{equation}
	where the last step is given by the L'Hopital rule. 
\end{proof}
\fi

\begin{lemma}
	\label{lemma:G(x)}
	Define $\tilde{G}(x)=  x-\Phi^{-1}(x)\cdot \phi \left[\Phi^{-1}(x)\right]$, where $x\in (0,1)$ is a continuous variable. We have
	\begin{equation*}
	\lim_{x\to 0} \tilde{G}(x) = 0,
	\end{equation*}
	and 
	\begin{equation*}
	\tilde{G}^\prime(x) = \left[\Phi^{-1}(x)\right]^2.
	\end{equation*}
	Therefore by the fundamental theorem of calculus,
	\begin{equation*}
	\tilde{G}(x) = \int_{0}^{x} \left[\Phi^{-1}(u)\right]^2 du.
	\end{equation*}
\end{lemma}
\begin{proof}
	First note that, since $\phi^\prime(v)= -v\cdot \phi(v)$ and $\lim_{v\to \pm \infty} \phi^\prime(v) =0$, we have
	\begin{equation*}
	\lim_{v\to \pm \infty} v \cdot \phi(v) = 0.
	\end{equation*}
	Let $v=\Phi^{-1}(x)$. Then
	\begin{equation*}
	\lim_{x\to 0} \tilde{G}(x)  = \lim_{v\to -\infty} -v \cdot \phi(v) = 0.
	\end{equation*}
	\iffalse
	and 
	\begin{equation}
	\lim_{x\to 1} \tilde{G}(x)  = 1- \lim_{v\to \infty} v \cdot \phi(v) = 1,
	\end{equation}
	\fi
	
	Next, we obtain the derivative of $\tilde{G}(x)$. Since $\Phi^\prime(x) = \phi(x)$, we have
	\begin{equation}
	\left[\Phi^{-1}(x)\right]^\prime = \frac{1}{\Phi^\prime \left[\Phi^{-1}(x)\right]}=\frac{1}{\phi \left[\Phi^{-1}(x)\right]}.
	\label{eq:G(x)_derivative_1}
	\end{equation}
	Also since $\phi^\prime(x) = -x\cdot \phi(x)$, we have
	\begin{equation}
	\phi^\prime\left[\Phi^{-1}(x)\right] = - \Phi^{-1}(x) \cdot \phi\left[\Phi^{-1}(x)\right] \cdot \left[\Phi^{-1}(x)\right]^\prime = -\Phi^{-1}(x).
	\label{eq:G(x)_derivative_2}
	\end{equation}
	By \eqref{eq:G(x)_derivative_1} and \eqref{eq:G(x)_derivative_2}, we have
	\begin{equation*}
	\tilde{G}^\prime(x) = 1 - \left[\Phi^{-1}(x)\right]^\prime \cdot  \phi\left[\Phi^{-1}(x)\right] - \left[\Phi^{-1}(x)\right] \cdot \phi^\prime\left[\Phi^{-1}(x)\right] =  \left[\Phi^{-1}(x)\right]^2.
	\end{equation*}
	
	Therefore, by the fundamental theorem of calculus, we have
	\begin{equation*}
	\tilde{G}(x) = \int_{0}^{x} \tilde{G}^\prime(u) du + \tilde{G}(0) = \int_{0}^{x} \left[\Phi^{-1}(u)\right]^2 du.
	\end{equation*}
\end{proof}

\begin{lemma}
	\label{lemma:sigmasq}
	Denote $\tilde{Q}$ as the quantile function of a $\chi_1^2$ distribution, and let $\tilde{H}(s) = -\tilde{Q}(1-s)$ where $s\in (0,1)$. For $0\le s \le t \le 1$, consider the truncated variance function
	\begin{equation}
	\tilde{\sigma}^2(s,t) = \int_{s}^{t} \int_{s}^{t} (u \wedge v -uv) d \tilde{H}(u) d \tilde{H}(v),
	\label{eq:sigmasq}
	\end{equation}
	where $u \wedge v =\min(u,v)$. We have
	\begin{equation*}
	0 \le \tilde{\sigma}^2(s,t) \le 1.
	\end{equation*}
\end{lemma}
\begin{proof}
	We first note three facts.
	\begin{align}
	\tilde{H}(s) &= -\left[\Phi^{-1}\left(1-\frac{s}{2}\right) \right]^2=-\left[\Phi^{-1}\left(\frac{s}{2}\right) \right]^2,\label{eq:Hs} \\
	d\tilde{H}(s) &= \frac{\Phi^{-1}(1-s/2)}{\phi\left[\Phi^{-1}(1-s/2)\right]}ds=-\frac{\Phi^{-1}(s/2)}{\phi\left[\Phi^{-1}(s/2)\right]}ds, \quad \text{by \eqref{eq:G(x)_derivative_1}},\label{eq:dH} \\
	\Phi^{-1}(w) &= -\sqrt{\log\frac{1}{w^2} - \log\log\frac{1}{w^2} - \log(2\pi)} + o(1), \quad \text{for small $w$, by \citetonline{Fung2017}}. \label{eq:Phiinv_order}
	\end{align}
	Hence for small $w$,
	\begin{equation}
	\label{eq:Phiinvsq_order}
	\left[\Phi^{-1}(w)\right]^2 = O\left(\log \frac{1}{w^2}\right).
	\end{equation}
	Then by \eqref{eq:Hs} and \eqref{eq:Phiinvsq_order}, we have
	\begin{equation}
	\label{eq:sHs_limit}
	\lim_{s \to 0} s\cdot \tilde{H}(s) = \lim_{s \to 0} -s\cdot \left[\Phi^{-1}\left(\frac{s}{2}\right)\right]^2 = 0.
	\end{equation}
	Also, by \eqref{eq:Hs} and Lemma \ref{lemma:G(x)},
	\begin{equation}
	\label{eq:Hs_integral}
	-\int_{0}^{x} \tilde{H}(s) ds =  2\cdot \tilde{G}\left(\frac{x}{2}\right).
	\end{equation}
	Since $u,v\in[0,1]$, we have $u \wedge v-uv \ge 0$. By \eqref{eq:dH}, we have $d\tilde{H}(s)/ds \ge 0$. Therefore, the integrand in \eqref{eq:sigmasq} is non-negative, so that
	\begin{equation*}
	\tilde{\sigma}^2(s,t) \ge 0,
	\end{equation*}
	and 
	\begin{equation*}
	\begin{aligned}
	\tilde{\sigma}^2(s,t) &\le \int_{0}^{1} \int_{0}^{1} (u \wedge v -uv) d \tilde{H}(u) d \tilde{H}(v),\\
	&= \int_{0}^{1} \left[\int_{0}^{v} u(1-v) d\tilde{H}(u) + \int_{v}^{1} v(1-u) d\tilde{H}(u)   \right]  d\tilde{H}(v),\\
	&= \int_{0}^{1} \left[\int_{0}^{v} u d\tilde{H}(u) + v\int_{v}^{1} d\tilde{H}(u) -v\int_{0}^{1}u d\tilde{H}(u)  \right]  d\tilde{H}(v).
	\end{aligned}
	\end{equation*}
	Denote 
	\begin{equation*}
	\tilde{M}(v) = \int_{0}^{v} u d\tilde{H}(u) + v\int_{v}^{1} d\tilde{H}(u) -v\int_{0}^{1}u d\tilde{H}(u).
	\end{equation*}
	Now, we consider the three integrals in $\tilde{M}(v)$. First note that
	\begin{equation*}
	\begin{aligned}
	\int_{0}^{x} u d\tilde{H}(u) &= u\cdot \tilde{H}(u) \Bigr|_{0}^x - \int_{0}^{x} \tilde{H}(u) du,\\
	&= x\cdot \tilde{H}(x) - \int_{0}^{x} \tilde{H}(u) du,\quad \text{by \eqref{eq:sHs_limit}}\\
	&=  x\cdot \tilde{H}(x) + 2\cdot \tilde{G}(x/2), \quad \text{by \eqref{eq:Hs_integral}}.
	\end{aligned}
	\end{equation*}
	It is easily verified that $\tilde{H}(1)=0$ and $\tilde{G}(1/2)=1/2$, we have
	\begin{equation*}
	\int_{0}^{1} u d\tilde{H}(u) = 2\cdot \tilde{G}(1/2)=1,
	\end{equation*}
	and
	\begin{equation*}
	v\int_{v}^{1} d\tilde{H}(u) = -v\cdot \tilde{H}(v).
	\end{equation*}
	Therefore,
	\begin{equation*}
	\begin{aligned}
	\tilde{M}(v) &= v\cdot \tilde{H}(v) +2 \cdot \tilde{G}(v/2)-v\cdot \tilde{H}(v)-2v\cdot \tilde{G}(1/2)\\
	&=2 \cdot \tilde{G}(v/2) - v.
	\end{aligned}
	\end{equation*}
	Finally,
	\begin{equation*}
	\begin{aligned}
	\int_{0}^{1} \tilde{M}(v) d\tilde{H}(v) &= \int_{0}^{1} 2 \cdot \tilde{G}(v/2)d\tilde{H}(v) - \int_{0}^{1}v d\tilde{H}(v),\\
	&= -\int_{0}^{1} \Phi^{-1}\left(\frac{v}{2}\right)\cdot \phi\left[\Phi^{-1}\left(\frac{v}{2}\right)\right]d\tilde{H}(v),\quad \text{by the definition of $\tilde{G}(x)$},\\
	&= 2\int_{0}^{1/2} \left[\Phi^{-1}(v)\right]^2 dv, \quad \text{by \eqref{eq:dH}},\\
	&=2\cdot \tilde{G}(1/2),\\
	&=1.
	\end{aligned}
	\end{equation*}
	Therefore,
	\begin{equation*}
	0 \le \tilde{\sigma}^2(s,t) \le 1.
	\end{equation*}
\end{proof}




\begin{theorem}
	\label{thm:ydf_representation}
	Assume the design matrix is orthogonal and the true model is null ($\mu=0$). Let $\tilde{X}_{(i)}$ be the $i$-th largest order statistic in an i.i.d sample of size $p$ from a $\chi^2_1$ distribution. Denote $\tilde{Y}_p = \tilde{\sigma}_p^{-1}(\sum_{i=1}^k \tilde{X}_{(i)} - \tilde{\mu}_p)$, where
	\begin{equation*}
	\tilde{\sigma}_p = \sqrt{p} \cdot \sigma(1/p,k/p),
	\end{equation*}
	and
	\begin{equation*}
	\tilde{\mu}_p = -p \int_{1/p}^{k/p} \tilde{H}(u) du - \tilde{H}\left(\frac{1}{p}\right),
	\end{equation*}
	where $\sigma(s,t)$ and $\tilde{H}(x)$ are defined in Lemma \ref{lemma:sigmasq}.
	
	As $k \to \infty$, $p \to \infty$ and $k=\left \lfloor{px}\right \rfloor$ with $x \in (0,1)$, we have
	\begin{equation}
	\frac{\text{df}_C(k)}{2p} = \frac{1}{2p} E\left[ \sum_{i=1}^k \tilde{X}_{(i)} \right]=  \frac{\tilde{\sigma}_p}{2p}E(\tilde{Y}_p) + \tilde{G}\left(\frac{k}{2p}\right) + O\left(\frac{\log(p)}{p}\right),
	\label{eq:ydf/2p_representation}
	\end{equation}
	where $\left \lfloor{\cdot}\right \rfloor$ denotes the greatest integer function, $\tilde{G}(x)$ is defined in Lemma \ref{lemma:G(x)}.
	
\end{theorem}
\begin{proof}
	We first apply a result in \citetonline{Csorgo1991}, to show that $\tilde{Y}_p=\tilde{\sigma}_p^{-1}(\sum_{i=1}^{k} \tilde{X}_{(i)}-\tilde{\mu}_p)$ converges in distribution to a standard normal. We then show how $\tilde{\mu}_p$ can be expressed in terms of function G plus a remainder term, which further leads to expression \eqref{eq:ydf/2p_representation}. 
	
	It follows from \citetonline{Csorgo1991} Corollary 2, that if there exist centering and normalizing constants $c_p$ and $d_p>0$, s.t.
	\begin{equation}
	d_p^{-1}(\tilde{X}_{(1)} - c_p) \xrightarrow{D} Y, \quad \text{where Y is the standard Gumbel distribution},
	\label{eq:scorgo_condition_gumbel} 
	\end{equation}
	then as $k \to \infty$, $p \to \infty$ and $k=\left \lfloor{px}\right \rfloor$ with $x \in (0,1)$,
	\begin{equation}
	\left(\sum_{i=1}^k \tilde{X}_{(i)}  - \tilde{\mu}_p\right) / \tilde{\sigma}_p \xrightarrow{D} Z, \quad \text{where Z is standard normal}.
	\label{eq:scorgo_result}
	\end{equation}
	
	% https://math.stackexchange.com/questions/450139/asymptotics-of-maxima-of-i-i-d-chi-square-random-variables
	First, it follows from \citetonline{Embrechts2013} that \eqref{eq:scorgo_condition_gumbel} holds, with $c_p=2\log(p)-\log\log(p)-\log(\pi)$ and $d_p=2$.
	\iffalse
	the maxima of $p$ gamma-distributed random variables, $\gamma(\alpha,\beta)$ with $\beta$ being the rate parameter, converges to a Gumbel distribution as $p \to \infty$, where the centering constant $c_p=\beta^{-1}(\log(p) +(\alpha-1)\log\log(p) - \log(\gamma(\alpha)))$, and the scaling constant $d_p=\beta^{-1}$. Meanwhile, we know that $\chi^2_1$ is a $\gamma(1/2,1/2)$ distribution. Therefore, \eqref{eq:scorgo_condition_gumbel} holds, where $c_p=2\log(p)-\log\log(p)-\log(\pi)$ and $d_p=2$.
	\fi
	
	Next, we have
	
	\begin{equation*}
	\begin{aligned}
	\tilde{\mu}_p &= -p \int_{1/p}^{k/p} \tilde{H}(u) du - \tilde{H}\left(\frac{1}{p}\right),\\
	&= -p \int_{0}^{k/p} \tilde{H}(u) du + p \int_{0}^{1/p} \tilde{H}(u) du - \tilde{H}\left(\frac{1}{p}\right),\\
	&= 2p\cdot \tilde{G}\left(\frac{k}{2p}\right) - 2p \cdot \tilde{G}\left(\frac{1}{2p}\right) + \left[\Phi^{-1}\left(\frac{1}{2p}\right)\right]^2, \quad \text{by \eqref{eq:Hs_integral}}.
	\end{aligned}		
	\end{equation*}
	Also, since
	\begin{equation*}
	\begin{aligned}
	\tilde{G}(\frac{1}{2p}) &= \frac{1}{2p} - \Phi^{-1}\left(\frac{1}{2p}\right) \cdot \phi\left[\Phi^{-1}\left(\frac{1}{2p}\right) \right],\quad \text{by definition of $\tilde{G}(x)$ in Lemma \ref{lemma:G(x)}},\\
	&= \frac{1}{2p} - \frac{1}{\sqrt{2\pi}}\Phi^{-1}\left(\frac{1}{2p}\right) \cdot \exp\left(-\frac{1}{2}\left[\Phi^{-1}\left(\frac{1}{2p}\right) \right]^2\right),\\
	&= \frac{1}{2p} + \frac{1}{\sqrt{2\pi}} \cdot \left(\sqrt{\log(4p^2)-\log\log(4p^2)-\log(2\pi)}+o(1)\right)\cdot \\
	& \qquad \exp\left[-\frac{1}{2} \left(\log(4p^2)-\log\log(4p^2)-\log(2\pi) +o(1)\right)\right], \quad \text{by \eqref{eq:Phiinv_order}},\\
	&= \frac{1}{2p} + \left(\sqrt{\log(4p^2)-\log\log(4p^2)-\log(2\pi)}+o(1)\right)\cdot \frac{\sqrt{\log(4p^2)}}{2p},\\
	&= O\left(\frac{\log(p)}{p}\right).
	\end{aligned}
	\end{equation*}
	Also
	\begin{equation*}
	\begin{aligned}
	\frac{1}{2p}\left[\Phi^{-1}\left(\frac{1}{2p}\right)\right]^2 &= O\left( \frac{\log(p)}{p}\right), \quad \text{by \eqref{eq:Phiinv_order}},
	\end{aligned}
	\end{equation*}
	and hence
	\begin{equation*}
	\begin{aligned}
	\frac{\tilde{\mu}_p}{2p} &= \tilde{G}\left(\frac{k}{2p}\right) - \tilde{G}\left(\frac{1}{2p}\right) + \frac{1}{2p}\left[\Phi^{-1}\left(\frac{1}{2p}\right)\right]^2,\\
	&= \tilde{G}\left(\frac{k}{2p}\right
	) + O\left( \frac{\log(p)}{p}\right).
	\end{aligned}
	\end{equation*}
	Therefore, \eqref{eq:ydf/2p_representation} holds, i.e.
	\begin{equation*}
	\frac{\text{df}_C(k)}{2p}=\frac{1}{2p} E\left(\sum_{i=1}^{k} \tilde{X}_{(i)}\right) = \frac{\tilde{\sigma}_p}{2p} E(\tilde{Y}_p) + \frac{\tilde{\mu}_p}{2p}=\frac{\tilde{\sigma}_p}{2p} E(\tilde{Y}_p) + \tilde{G}\left(\frac{k}{2p}\right) + O\left( \frac{\log(p)}{p}\right).
	\end{equation*}
\end{proof}

\begin{corollary}
	\label{corollary:Yp_order}
	If $\limsup |E(\tilde{Y_p})| < \infty$, we further have:
	\begin{equation}
	\frac{\text{df}_C(k)}{2p} = \tilde{G}\left(\frac{k}{2p}\right) + O\left(\frac{\log(p)}{p}\right) + O\left(\frac{1}{\sqrt{p}}\right).
	\label{eq:ydf/2p_representation_remark}
	\end{equation}
\end{corollary}
\begin{proof}
	By Lemma \ref{lemma:sigmasq} we have $0 \le \sigma(1/p,k/p) \le 1$, and hence $\tilde{\sigma}_p = O(\sqrt{p})$. Therefore by Theorem \ref{thm:ydf_representation}, we have
	\begin{equation*}
	\frac{\text{df}_C(k)}{2p}=  \tilde{G}\left(\frac{k}{2p}\right) + O\left( \frac{\log(p)}{p}\right) + O\left(\frac{1}{\sqrt{p}}\right).
	\end{equation*}	
\end{proof}

\dfasy*

\begin{proof}
	By Lemma \ref{lemma:hdf_nulltrue}, we have
	\begin{equation*}
	\text{hdf}(k) = df_L(\tilde{M}^{-1}(k)) = k - 2p\cdot \Phi^{-1} \left(\frac{k}{2p}\right) \cdot \phi\left[\Phi^{-1}\left(\frac{k}{2p}\right) \right].
	\end{equation*}
	Then by the definition of $\tilde{G}(x)$ in Lemma \ref{lemma:G(x)},
	\begin{equation*}
	\frac{1}{2p} \text{hdf}(k) = \tilde{G}\left(\frac{k}{2p}\right).
	\end{equation*}
	By Theorem \ref{thm:ydf_representation}, we also have
	\begin{equation*}
	\frac{1}{2p}\text{df}_C(k) = \frac{\sigma_p}{2p}E(\tilde{Y}_p) + \tilde{G}\left(\frac{k}{2p}\right) + O\left(\frac{\log(p)}{p} \right).
	\end{equation*}
	Therefore, \eqref{eq:hdf_ydf_yp_representation} holds, i.e.
	\begin{equation*}
	\frac{1}{2p} \text{hdf}(k) = \frac{1}{2p}\text{df}_C(k) - \frac{\tilde{\sigma}_p}{2p}E(\tilde{Y}_p) + O\left(\frac{\log(p)}{p} \right).
	\end{equation*}
\end{proof}

\dfasycorollary*
\begin{proof}
	By Theorem \ref{thm:hdf_ydf_representation} and Corollary \ref{corollary:Yp_order},
	\begin{equation*}
	\frac{1}{2p} \text{hdf}(k) = \frac{1}{2p}\text{df}_C(k) + O\left(\frac{1}{\sqrt{p}}\right) + O\left(\frac{\log(p)}{p} \right).
	\end{equation*}
	From Lemma \ref{lemma:G(x)}, $\tilde{G}(x)$ is a non-decreasing function with $\tilde{G}(0+)=0$ and $\tilde{G}(1/2)=1/2$. Thus, 
	\begin{equation*}
	\frac{2p}{\text{hdf}(k)} = \frac{1}{\tilde{G}\left(\frac{k}{2p}\right)} = O(1),
	\end{equation*}
	since $k=\left \lfloor{px}\right \rfloor$ and $x\in(0,1)$. Therefore, 
	\begin{equation*}
	\frac{\text{df}_C(k)}{\text{hdf}(k)} = 1 + O\left(\frac{1}{\sqrt{p}}\right) + O\left(\frac{\log(p)}{p} \right),
	\end{equation*}
	and hence
	\begin{equation*}
	\frac{\text{df}_C(k)}{\text{hdf}(k)} \to 1.
	\end{equation*}
\end{proof}

\subsection{Expected KL-based optimism, in the context of BS }
\label{sec:expectedkl_bs}
In this section, we obtain the expected Kullback-Leibler (KL) based optimism for BS with subset size $k$. Let's first consider fitting least squares regression on $k$ prefixed predictors. Recall that 
\begin{equation*}
y = \mu + \epsilon,
\end{equation*}
where $\epsilon \sim \mathcal{N}(0,\sigma^2 I)$. We use the deviance to measure the predictive error, that is 
\begin{equation*}
\Theta=-2 \log f(y|\mu,\sigma^2).
\end{equation*}
The training error is then 
$$\text{err}_{\text{KL}} = -2 \log f (y|\hat{\mu},\hat{\sigma}^2),$$
and the testing error (KL information) is
$$\text{Err}_{\text{KL}}  = -2 E_0 \left[ \log f(y^0|\hat{\mu},\hat{\sigma}^2)\right],$$
where $\hat{\mu}$ and $\hat{\sigma}^2$ are the maximum likelihood estimators (MLE) based on training data $(X,y)$, $y^0$ is independent and has the same distribution of $y$ and $E_0$ is the expectation over $y^0$. 

Due to the assumption of normality, the deviance can be expressed as
\begin{equation}
\Theta = n\log(2\pi \sigma^2) + \frac{\lVert y- \mu \rVert_2^2}{\sigma^2}.
\label{eq:deviance}
\end{equation}
Maximizing the likelihood, or minimizing the deviance \eqref{eq:deviance}, gives
\begin{equation}
\begin{aligned}
& \hat{\mu} = \argmin_\mu  \lVert y-\mu \rVert_2^2,\\
&\hat{\sigma}^2 = \frac{1}{n} \lVert y-\hat{\mu}\rVert_2^2.
\label{eq:appen_mle}
\end{aligned}
\end{equation}
Using these expressions, we then have
\begin{equation}
\text{err}_\text{KL} = n \log(2\pi \hat{\sigma}^2) +n,
\label{eq:err_kl}
\end{equation}
and
\begin{equation*}
\text{Err}_\text{KL} = n\log(2\pi \hat{\sigma}^2) + n\frac{\sigma^2}{\hat{\sigma}^2} +\frac{\lVert \mu- \hat{\mu} \rVert_2^2}{\hat{\sigma}^2}.
\end{equation*}
The expected optimism is then
\begin{equation}
\begin{aligned}
E(\text{op}_\text{KL})  &= E(\text{Err}_\text{KL}) - E(\text{err}_\text{KL}),\\
&= E\left(n\frac{\sigma^2}{\hat{\sigma}^2}\right) + E\left(\frac{\lVert \mu- \hat{\mu}) \rVert_2^2}{\hat{\sigma}^2}\right) -n.
\end{aligned}
\label{eq:eop}
\end{equation}

So far we've been considering a subset with $k$ fixed predictors. At subset size $k$, BS chooses the one with minimum residual sum of squares (RSS) from all $\binom{p}{k}$ possible subsets. In order for the above derivation to continue to hold for BS of subset size $k$, we need to show that the MLE from \eqref{eq:appen_mle} is also the BS fit. This can be easily obtained from the full likelihood (-2 times) \eqref{eq:err_kl}, which after substituting the expression of $\hat{\sigma}$ leads to
\begin{equation*}
n\log\left(\frac{2\pi}{n}\lVert y-\hat{\mu}\rVert_2^2\right) + n.
\end{equation*}
Therefore, for all $\binom{p}{k}$ models of size $k$, the one with largest log likelihood, is also the one with smallest RSS. Hence \eqref{eq:eop} holds for BS fit with subset size $k$ as well.

\subsection{Proof of Theorem \ref{thm:correspondence}}
\label{sec:correspondence}

\begin{proof}	
	Since $[X_1,X_2,\cdots,X_j]$ and $[Q_1,Q_2,\cdots,Q_j]$ span the same space, we have
	\begin{equation}
	\hat{\alpha}^{(j)} = \hat{\beta}^{(j)}.
	\label{eq:thmproof-correspondence-zrq-zrx-subset}
	\end{equation}
	We can express $\hat{\gamma}(k_Q)$ as
	\begin{equation}
		\hat{\gamma}(k_Q) = \sum_{j\in S_k} \hat{\gamma}^{(j)} - \hat{\gamma}^{(j-1)}.
		\label{eq:zs_expand}
	\end{equation}
	We multiply both sides by $R^{-1}$ ($X$ is assumed to have full column rank), and use \eqref{eq:thmproof-correspondence-zrq-zrx-subset} to get
	\begin{equation*}
		\hat{\beta}(k_Q) = \sum_{j\in S_k} \hat{\alpha}^{(j)} - \hat{\alpha}^{(j-1)}.
		%\label{eq:thmproof-correspondence-conclusion}
	\end{equation*}
	\iffalse
	 \eqref{eq:thmproof-correspondence-conclusion} tells us that when certain subset $Q_S$ is chosen, the coefficients projected from the $Q$ space, correspond to a linear combination of multiple regression coefficients of $y$ upon subsets in $X$, where these subsets are sequential. For example, in the simple $2$-predictor case, if $Q_2$ is the chosen predictor, by \eqref{eq:thmproof-correspondence-conclusion}, we get:
	\begin{equation*}
	\hat{\beta}^{(Q_2)} = \hat{\beta}^{(X_1,X_2)} - \hat{\beta}^{(X_1)}.
	\end{equation*}
	Hence, it corresponds to the difference between two regression coefficients, the coefficients of $y$ upon $X_1,X_2$, and the coefficients of $y$ upon just $X_1$. 
	\fi
\end{proof}


\clearpage
\topmargin= -0.4in
\textheight = +8.9in
\oddsidemargin = 0.05in
\evensidemargin = 0.05in
\textwidth = 6.5in

\spacingset{1.2} % DON'T change the spacing!

\section{Complete simulation results}
\begin{itemize}
	\item Orthogonal $X$, simulation setups are discussed in Section \ref{sec:simulation_setup_orthx}. 
	\begin{itemize}
		\item The performance of selection rules for BS. The selection rules include C$_p$, AICc, BIC, GCV and 10-fold CV. For each selection rule except CV, there are two columns in the table indicating the degrees of freedoms to use in calculating the information criterion. The ``edf'' (effective degrees of freedom) is estimated using definition \eqref{eq:edf} by assuming the knowledge of $\mu$ and $\sigma$, and hence it is an infeasible rule. The ``ndf/hdf/bdf'' (naive degrees of freedom /  heuristic degrees of freedom / degrees of freedom based on bootstrap) are feasible selection rules in practice. 
		\begin{itemize}
			\item Orth-Sparse-Ex1: tables S1-S2
			\item Orth-Sparse-Ex2: tables S3-S4
			\item Orth-Dense: tables S5-S6
		\end{itemize}
		\item The performance of BS and regularization methods. Note that for lasso, we use the number of non-zero coefficients $k(\lambda)$ in place of edf in the AICc formula \eqref{eq:aicc_edf}. \citetonline{Zou2007} showed that $k(\lambda)$ is an unbiased estimator of edf for lasso. For gamma lasso, \citetonline{Taddy2017} suggested a heuristic degrees of freedom to be plugged into \eqref{eq:aicc_edf} in order to use AICc as the selection rule.
		\begin{itemize}
			\item Orth-Sparse-Ex1: tables S7-S8
			\item Orth-Sparse-Ex2: tables S9-S10
			\item Orth-Dense: tables S11-S12
		\end{itemize}
	\end{itemize}
	\item General $X$, simulation setups are discussed in Section \ref{sec:simulation_setup_generalx}. 
	\begin{itemize}
		\item The performance of BOSS, BS, FS, lasso, gamma lasso, SparseNet and relaxed lasso (rlasso).
		\begin{itemize}
			\item Sparse-Ex1: tables S13-S18
			\item Sparse-Ex2: tables S19-S24
			\item Sparse-Ex3: tables S25-S30
			\item Sparse-Ex4: tables S31-S36
			\item Dense: tables S37-S42
		\end{itemize}
	\end{itemize}
\end{itemize}

\clearpage

\input{tables/supplement/bs_ic/Orth-Sparse-Ex1_n200.tex}
\input{tables/supplement/bs_ic/Orth-Sparse-Ex1_n2000.tex}
\input{tables/supplement/bs_ic/Orth-Sparse-Ex2_n200.tex}
\input{tables/supplement/bs_ic/Orth-Sparse-Ex2_n2000.tex}
\input{tables/supplement/bs_ic/Orth-Dense_n200.tex}
\input{tables/supplement/bs_ic/Orth-Dense_n2000.tex}


\input{tables/supplement/bs_regu/Orth-Sparse-Ex1_n200.tex}
\input{tables/supplement/bs_regu/Orth-Sparse-Ex1_n2000.tex}
\input{tables/supplement/bs_regu/Orth-Sparse-Ex2_n200.tex}
\input{tables/supplement/bs_regu/Orth-Sparse-Ex2_n2000.tex}
\input{tables/supplement/bs_regu/Orth-Dense_n200.tex}
\input{tables/supplement/bs_regu/Orth-Dense_n2000.tex}

\input{tables/supplement/boss/Sparse-Ex1_n200_rho_0.tex}
\input{tables/supplement/boss/Sparse-Ex1_n2000_rho_0.tex}
\input{tables/supplement/boss/Sparse-Ex1_n200_rho_05.tex}
\input{tables/supplement/boss/Sparse-Ex1_n2000_rho_05.tex}
\input{tables/supplement/boss/Sparse-Ex1_n200_rho_09.tex}
\input{tables/supplement/boss/Sparse-Ex1_n2000_rho_09.tex}
\input{tables/supplement/boss/Sparse-Ex2_n200_rho_0.tex}
\input{tables/supplement/boss/Sparse-Ex2_n2000_rho_0.tex}
\input{tables/supplement/boss/Sparse-Ex2_n200_rho_05.tex}
\input{tables/supplement/boss/Sparse-Ex2_n2000_rho_05.tex}
\input{tables/supplement/boss/Sparse-Ex2_n200_rho_09.tex}
\input{tables/supplement/boss/Sparse-Ex2_n2000_rho_09.tex}
\input{tables/supplement/boss/Sparse-Ex3_n200_rho_0.tex}
\input{tables/supplement/boss/Sparse-Ex3_n2000_rho_0.tex}
\input{tables/supplement/boss/Sparse-Ex3_n200_rho_05.tex}
\input{tables/supplement/boss/Sparse-Ex3_n2000_rho_05.tex}
\input{tables/supplement/boss/Sparse-Ex3_n200_rho_09.tex}
\input{tables/supplement/boss/Sparse-Ex3_n2000_rho_09.tex}
\input{tables/supplement/boss/Sparse-Ex4_n200_rho_0.tex}
\input{tables/supplement/boss/Sparse-Ex4_n2000_rho_0.tex}
\input{tables/supplement/boss/Sparse-Ex4_n200_rho_05.tex}
\input{tables/supplement/boss/Sparse-Ex4_n2000_rho_05.tex}
\input{tables/supplement/boss/Sparse-Ex4_n200_rho_09.tex}
\input{tables/supplement/boss/Sparse-Ex4_n2000_rho_09.tex}
\input{tables/supplement/boss/Dense_n200_rho_0.tex}
\input{tables/supplement/boss/Dense_n2000_rho_0.tex}
\input{tables/supplement/boss/Dense_n200_rho_05.tex}
\input{tables/supplement/boss/Dense_n2000_rho_05.tex}
\input{tables/supplement/boss/Dense_n200_rho_09.tex}
\input{tables/supplement/boss/Dense_n2000_rho_09.tex}



\clearpage
\bibliographystyleonline{chicago}
\bibliographyonline{reference.bib}