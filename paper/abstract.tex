 %!TEX root = boss.tex
\begin{abstract}
  Least squares (LS) based subset selection methods are popular in linear regression modeling when the number of predictors is less than the number of observations. Best subset selection (BS) is known to be NP hard and has a computational cost that grows exponentially with the number of predictors. Forward stepwise selection (FS) is a greedy heuristic for BS. Both methods rely on cross-validation (CV) in order to select the subset size $k$, which requires fitting the procedures multiple times and results in a selected $k$ that is random across replications. Compared to CV, information criteria only require fitting the procedures once, and we show that for LS-based methods they can result in better predictive performance while providing a non-random choice of $k$. However, information criteria require knowledge of the effective degrees of freedom for the fitting procedure, which is generally not available analytically for complex methods. In this paper, we propose a novel LS-based method, the best orthogonalized subset selection (BOSS) method, which performs BS upon an orthogonalized basis of ordered predictors. Assuming orthogonal predictors, we build a connection between BS and its Lagrangian formulation (i.e., minimization of the residual sum of squares plus the product of a regularization parameter and $k$), and based on this connection introduce a heuristic degrees of freedom (hdf) for BOSS that can be estimated via an analytically-based expression. We show in both simulations and real data analysis that BOSS using the Kullback-Leibler based information criterion AICc-hdf has the strongest performance of all of the LS-based methods considered and is competitive with regularization methods, with the computational effort of a single ordinary LS fit. Supplementary materials, an R package \pkg{BOSSreg} and the computer code to reproduce the results for this article are available online.

\end{abstract}


\iffalse
\begin{abstract}
  Best subset selection (BS) is a popular least squares (LS) based subset selection method for regression modeling. In order to select the optimal subset size, one often applies an information criterion such as the Mallows' C$_p$. \citet{Ye1998} and \citet{Efron2004} demonstrated that with the effective degrees of freedom (edf) being plugged in place of the subset size, C$_p$-edf provides an unbiased estimate of the prediction error. This paper is largely motivated by the following challenges of applying BS in practice: (1) BS is NP-hard and its computational cost grows exponentially with the problem size; and (2) the edf for BS generally does not have an analytical expression. In this paper, we start from a restricted case (orthogonal design matrix $X$). We build a connection between BS and its Lagrangian version, and propose a heuristic degrees of freedom (hdf) for BS, which can be estimated via an analytically-based expression. Furthermore, we introduce AICc-edf and its feasible version AICc-hdf, which are motivated by trying to construct an unbiased estimator of the Kullback-Leibler divergence for BS. Finally, we return to a general $X$ and propose a novel LS-based method, the best orthogonalized subset selection (BOSS) method. BOSS along with AICc-hdf works well in both simulations and real data analysis, with the computational effort of a single ordinary LS fit. 
\end{abstract}
\fi


\noindent%
{\it Keywords:} Least squares; Best subset selection; Effective degrees of freedom; Information criteria; Cross validation.
\vfill