%!TEX root = ms.tex
\section{Real data analysis}
\label{sec:real_data}
In this section, we implement BOSS on five real world datasets. We consider four datasets from the StatLib library\footnote{http://lib.stat.cmu.edu/datasets/}, which is maintained at Carnegie Mellon University. The ``Housing'' data are often used in comparisons of different regression methods. The aim is to predict the housing values in the suburbs of Boston based on $13$ predictors, including crime rate, property tax rate, pupil-teacher ratio, etc. The ``Hitters'' data contain the 1987 annual salary for MLB players. For each player, it records $19$ different performance metrics happening in 1986, such as number of times at bat, number of hits, etc., and the task is to predict the salary based on these statistics. The ``Auto'' data are driven by prediction of the miles per gallon of vehicles based on features like the horsepower, weight, etc. The ``College'' data contain various statistics for a large number of US colleges from the 1995 issue of ``US News and World Report'', and we use these statistics to predict the number of applications received. We also consider a dataset from the Machine Learning Repository\footnote{https://archive.ics.uci.edu/ml} that is maintained by UC Irvine. The ``ForestFire'' data are provided by \citet{cortez2007data} and the aim is to use meteorological and other data to predict the burned area of forest fires that happened in the northeast region of Portugal. The authors considered several machine learning algorithms, e.g. support vector regression, and concluded that the best prediction in terms of RMSE is the naive mean vector.

%The 'AirFoil' data has different size airfoils at different wind tunnel speeds and angles of attack, based on experiments performed by NASA, and the purpose is to predict sound pressure level. 

In real data analysis, one almost always would consider an intercept term. The way that BOSS handles the intercept term is described in Algorithm \ref{alg:boss}. To be more specific, we first center both $X$ and $y$, and fit BOSS-AICc-hdf without an intercept to get $\hat{\beta}$. Then we calculate the intercept by $
\hat{\beta}_0=\bar{y} - \bar{X}^T \hat{\beta}$, which can be easily shown to be equivalent to fitting an intercept in every subset considered by BOSS. 

We compare the performance of BOSS with LS-based methods BS and FS, and with regularization methods lasso and SparseNet. All of the methods are fitted with an intercept term. Note that for the Forest Fires dataset, we fit BS via MIO \citep{Bertsimas2016} using the {\tt{R}} package \pkg{bestsubset} \citep{Hastie2017}, where we restrict subset size $k=0,\dots,10$, with $3$ minutes as the time budget to find an optimal solution for each $k$, as suggested by the authors. For all of the other datasets, BS is fitted using the \pkg{leaps} package. To measure the performance of each method, we apply the leave-one-out (LOO) testing procedure, in which we fit the method on all observations except one, test the performance on that particular observation, and repeat the procedure for all $n$ observations. 

Table \ref{tab:realdata} presents the average RMSE, the average number of predictors and average running time for various methods given by LOO testing. We see that BOSS provides the best predictive performance in all datasets except the ``Housing'' and ``College'' data where lasso is the best for those datasets and its RMSE is $0.3\%$ and $0.04\%$ lower than those of BOSS, respectively. Due to a highly optimized implementation of the cyclical coordinate descent, the ``glmnet'' algorithm is extremely fast to provide the lasso solution. BS is still not scalable to large dimensions, even by using the modern tools. With the dimension $p=55$, it takes around $350$ seconds to perform 10-fold CV for subset sizes restricted to be no greater than $10$. However, We observe that BOSS is reasonably computationally efficient and much faster than BS, FS and SparseNet. 

% tab:realdata
\input{tables/realdata.tex}



