%!TEX root = ms.tex
\section{Conclusion and future work}
\label{sec:conclusion}
In this paper, we propose an LS-based subset selection method BOSS. In order to select the single optimal subset, we introduce a heuristic degrees of freedom (hdf) for BOSS and a KL-based information criterion AICc-hdf. BOSS together with AICc-hdf has computational cost on the same order as a single LS fit. We find that AICc-hdf is the best selection rule compared to other information criteria and CV. We further show in simulations and real data examples that BOSS using AICc-hdf is competitive in both speed and predictive performance. Finally, we provide justifications of hdf and AICc in a restricted scenario where $X$ is orthogonal.

The strong performance of BOSS using AICc compared to using CV suggests that the pursuit of methods to approximate edf (which normally does not have an analytical expression for complex modeling methods and algorithms), particularly for methods that are more sensitive to small perturbations in the data, is worthy of further research. Also, we focus on a fixed $X$ in this paper, however, in many applications where the data are observational and the experiment is performed in an uncontrolled manner, it is more appropriate to treat $X$ as random. It is interesting to study the performance of BOSS using an information criterion designed for random predictors, e.g. RAICc \citep{tian2020selection}.